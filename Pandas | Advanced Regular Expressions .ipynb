{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas | Advanced Regular Expressions\n",
    "---\n",
    "## Concepts:  \n",
    "- Using multiple capture groups to extract URL data.\n",
    "- How to use lookarounds to customize matches based on the surrounding text.\n",
    "- How to substitute a regular expression match to clean inconsistent data.\n",
    "- How to use named capture groups to extract dataframes from a text column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We're going to continue working with the dataset from the previous mission from technology site Hacker News. Let's take a moment to refresh our memory of the different columns in this dataset:\n",
    "\n",
    "- `id:` The unique identifier from Hacker News for the story\n",
    "- `title`: The title of the story\n",
    "- `url`: The URL that the stories links to, if the story has a URL\n",
    "- `num_points`: The number of points the story acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "- `num_comments`: The number of comments that were made on the story\n",
    "- `author`: The username of the person who submitted the story\n",
    "- `created_at`: The date and time at which the story was submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = pd.read_csv(\"hacker_news.csv\")\n",
    "titles = hn['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We have already imported pandas and re, read the CSV and extracted the title column.\n",
    "\n",
    "1. Create a case insensitive regex pattern that matches all case variations of the letters `SQL`.\n",
    "2. Use that regex pattern and the ignorecase flag to count the number of mentions of `SQL` in titles. Assign the result to `sql_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"sql\"\n",
    "sql_counts = titles.str.contains(pattern, flags=re.I).sum()\n",
    "print(sql_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We have created a new dataframe, __`hn_sql`__, including only rows that mention a SQL flavor.\n",
    "\n",
    "1. Create a new column called flavor in the __`hn_sql`__ dataframe, containing extracted mentions of SQL flavors, defined as:\n",
    "    - Any time 'SQL' is preceded by one or more word characters.\n",
    "    - Ignoring all case variation.\n",
    "2. Use the __`Series.str.lower()`__ method to clean the values in the flavor column by converting them to lowercase. Assign the values back to the column in __`hn_sql`__.\n",
    "3. Use the __`DataFrame.pivot_table()`__ method to create a pivot table, __`sql_pivot`__.\n",
    "4. The index of the pivot table should be the __`flavor`__ column.\n",
    "5. The values of the pivot table should be the mean of the __`num_comments`__ column, aggregated by SQL flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_sql = hn[hn['title'].str.contains(r\"\\w+SQL\", flags=re.I)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(\\w+sql)\"\n",
    "flavor = hn_sql.title.str.extract(pattern, flags=re.I)\n",
    "hn_sql['flavor'] = flavor\n",
    "hn_sql.flavor = hn_sql.flavor.str.lower()\n",
    "\n",
    "sql_pivot = hn_sql.pivot_table('num_comments', 'flavor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavor</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cloudsql</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>memsql</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mysql</td>\n",
       "      <td>12.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nosql</td>\n",
       "      <td>14.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postgresql</td>\n",
       "      <td>25.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sparksql</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            num_comments\n",
       "flavor                  \n",
       "cloudsql        5.000000\n",
       "memsql         14.000000\n",
       "mysql          12.230769\n",
       "nosql          14.529412\n",
       "postgresql     25.962963\n",
       "sparksql        1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Write a regular expression pattern which will match `Python` or `python`, followed by a space, followed by one or more digit characters or periods.\n",
    "    - The regular expression should contain a capture group for the digit and period characters (the Python versions)\n",
    "2. Extract the Python versions from `title` using the regular expression pattern.\n",
    "3. Use `Series.value_counts()` and the `dict()` function to create a dictionary frequency table of the extracted Python versions. Assign the result to `py_versions_freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[Pp]ython ([\\d\\.]+)\"\n",
    "\n",
    "py_versions = titles.str.extract(pattern)\n",
    "py_versions_freq = dict(py_versions[0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 10,\n",
       " '2': 3,\n",
       " '3.5': 3,\n",
       " '3.6': 2,\n",
       " '8': 1,\n",
       " '1.5': 1,\n",
       " '2.7': 1,\n",
       " '4': 1,\n",
       " '3.5.0': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_versions_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We have provided a commented line of code containing the regular expression we used above.\n",
    "\n",
    "1. Uncomment the line of code. Add a negative set to the end of the regular expression that excludes:\n",
    "    - The period character `.`\n",
    "    - The plus character `+`.\n",
    "2. Use the `first_10_matches()` function to return the matches for the regular expression you built, assigning the result to `first_ten`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_10_matches(pattern):\n",
    "    \"\"\"\n",
    "    Return the first 10 story titles that match\n",
    "    the provided regular expression\n",
    "    \"\"\"\n",
    "    all_matches = titles[titles.str.contains(pattern)]\n",
    "    first_10 = all_matches.head(10)\n",
    "    return first_10\n",
    "pattern_z = r\"\\b[Cc]\\b[^+.]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#  Lookarounds `?=` |  `?!`  | `?<=` | `?<!`   \n",
    "Let's look at the result of the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365                      The new C standards are worth it\n",
       "444           Moz raises $10m Series C from Foundry Group\n",
       "521          Fuchsia: Micro kernel written in C by Google\n",
       "1307            Show HN: Yupp, yet another C preprocessor\n",
       "1326                     The C standard formalized in Coq\n",
       "1365                          GNU C Library 2.23 released\n",
       "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
       "1620                        SDCC  Small Device C Compiler\n",
       "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
       "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_10_matches(pattern_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = ['Red_Green_Blue',\n",
    "              'Yellow_Green_Red',\n",
    "              'Red_Green_Red',\n",
    "              'Yellow_Green_Blue',\n",
    "              'Green']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a function that will loop over our test cases and tell us whether our pattern matches. We'll use the re module rather than pandas since it tells us the exact text that matches, which will help us understand how the lookaround is working:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_cases(pattern):\n",
    "    for tc in test_cases:\n",
    "        result = re.search(pattern, tc)\n",
    "        print(result or \"NO MATCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive lookahead  `?=`\n",
    "---\n",
    "In each instance, we'll aim to match the substring __`Green`__ depending on the characters that precede or follow it. Let's start by using a __positive lookahead__ to include instances where the match is followed by the substring __`_Blue`__. We'll include the underscore character in the lookahead, otherwise we will get zero matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "NO MATCH\n",
      "<re.Match object; span=(7, 12), match='Green'>\n",
      "NO MATCH\n"
     ]
    }
   ],
   "source": [
    "#'Red_Green_Blue'\n",
    "#'Yellow_Green_Red'\n",
    "#'Red_Green_Red'\n",
    "#'Yellow_Green_Blue'\n",
    "#'Green'\n",
    "\n",
    "run_test_cases(r\"Green(?=_Blue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative lookahead  `?!`\n",
    "---\n",
    "Notice how the matches themselves are purely the text `Green` and don't include the lookahead. Let's look at a negative lookahead to include instances where the match is not followed by the substring `_Red`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "NO MATCH\n",
      "<re.Match object; span=(7, 12), match='Green'>\n",
      "<re.Match object; span=(0, 5), match='Green'>\n"
     ]
    }
   ],
   "source": [
    "#'Red_Green_Blue'\n",
    "#'Yellow_Green_Red'\n",
    "#'Red_Green_Red'\n",
    "#'Yellow_Green_Blue'\n",
    "#'Green'\n",
    "\n",
    "run_test_cases(r\"Green(?!_Red)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive lookbehind `?<=`\n",
    "Next we'll use a __positive lookbehind__ to include instances where the match is preceded by the substring `Red_`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "NO MATCH\n"
     ]
    }
   ],
   "source": [
    "#'Red_Green_Blue'\n",
    "#'Yellow_Green_Red'\n",
    "#'Red_Green_Red'\n",
    "#'Yellow_Green_Blue'\n",
    "#'Green'\n",
    "\n",
    "run_test_cases(r\"(?<=Red_)Green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative lookbehind `?<!`\n",
    "And finally, using a __negative lookbehind__ to include instances where the match isn't preceded by the substring `Yellow_`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "<re.Match object; span=(0, 5), match='Green'>\n"
     ]
    }
   ],
   "source": [
    "#'Red_Green_Blue'\n",
    "#'Yellow_Green_Red'\n",
    "#'Red_Green_Red'\n",
    "#'Yellow_Green_Blue'\n",
    "#'Green'\n",
    "\n",
    "run_test_cases(r\"(?<!Yellow_)Green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of a lookaround can include any other regular expression component. For instance, here is an example where we __match only cases that are followed by exactly five characters__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='Green'>\n",
      "NO MATCH\n",
      "NO MATCH\n",
      "<re.Match object; span=(7, 12), match='Green'>\n",
      "NO MATCH\n"
     ]
    }
   ],
   "source": [
    "run_test_cases(r\"Green(?=.{5})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third test cases are followed by four characters, not five, and the last test case isn't followed by anything.\n",
    "\n",
    "Sometimes programming languages won't implement support for all lookarounds (__notably, lookbehinds are not in the official JavaScript specification__). As an example, to get full support in the RegExr tool, you'll need to set it to use the PCRE regex engine.\n",
    "\n",
    "In this exercise, we're going to use lookarounds to refine the regular expression we build on the last screen to capture mentions of the \"C\" programming language. As a reminder, here is the last of the regular expressions we attempted to use with this exercise earlier, and the resultant titles that match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365                      The new C standards are worth it\n",
       "444           Moz raises $10m Series C from Foundry Group\n",
       "521          Fuchsia: Micro kernel written in C by Google\n",
       "1307            Show HN: Yupp, yet another C preprocessor\n",
       "1326                     The C standard formalized in Coq\n",
       "1365                          GNU C Library 2.23 released\n",
       "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
       "1620                        SDCC  Small Device C Compiler\n",
       "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
       "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_10_matches(r\"\\b[Cc]\\b[^.+]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Write a regular expression and assign it to `pattern`. \n",
    "    The regular expression should:\n",
    "    - Match instances of `C` or `c` where they are not preceded or followed by another letter.\n",
    "    - Exclude instances where the match is followed by a `.` or `+` character, without removing instances where the match occurs at the end of the string.\n",
    "    - Exclude instances where the word `'Series'` immediately precedes the match.\n",
    "2. Count how many stories in `titles` match the regular expression. Assign the result to `c_mentions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?<!Series\\s)\\b[Cc]\\b(?![\\+\\.])\"\n",
    "c_mentions = titles.str.contains(pattern).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365                      The new C standards are worth it\n",
       "521          Fuchsia: Micro kernel written in C by Google\n",
       "1307            Show HN: Yupp, yet another C preprocessor\n",
       "1326                     The C standard formalized in Coq\n",
       "1365                          GNU C Library 2.23 released\n",
       "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
       "1620                        SDCC  Small Device C Compiler\n",
       "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
       "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
       "2589    Phalcon  PHP framework delivered as a C extension\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_10_matches(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mentions = titles.str.contains(pattern).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(21, 23), match='oo'>\n",
      "<re.Match object; span=(2, 4), match='ee'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(13, 15), match='ee'>\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "              \"I'm going to read a book.\",\n",
    "              \"Green is my favorite color.\",\n",
    "              \"My name is Aaron.\",\n",
    "              \"No doubles here.\",\n",
    "              \"I have a pet eel.\"\n",
    "             ]\n",
    "\n",
    "for tc in test_cases:\n",
    "    print(re.search(r\"(\\w)\\1\", tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there was no match for the word __`Aaron`__, despite it containing a double __`\"a.\"`__ This is because the uppercase and lowercase __`\"a\"`__ are two different characters, so the backreference does not match.\n",
    "\n",
    "We can easily achieve the same thing using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4     True\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_cases = pd.Series(test_cases)\n",
    "print(test_cases.str.contains(r\"(\\w)\\1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this technique to identify story titles that have repeated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Write a regular expression to match cases of __repeated words__:\n",
    "    - We'll define a word as a series of one or more word characters that are preceded and followed by a boundary anchor.\n",
    "    - We'll define repeated words as the same word repeated twice, separated by a whitespace character.\n",
    "2. Select only the items in `titles` that match the regular expression. Assign the result to `repeated_words.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_dw = r\"\\b(\\w+)\\s\\1\\b\"\n",
    "repeated_words = titles[titles.str.contains(pattern_dw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We have provided `email_variations`, a pandas Series containing all the variations of \"email\" in the dataset.\n",
    "1. Use a regular expression to replace each of the matches in `email_variations` with `\"email\"` and assign the result to `email_uniform`.\n",
    "    - You may need to iterate several times when writing your regular expression in order to match every item.\n",
    "2. Use the same syntax to replace all mentions of email in `titles` with `\"email\"`. Assign the result to `titles_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_variations = pd.Series(['email', 'Email', 'e Mail',\n",
    "                        'e mail', 'E-mail', 'e-mail',\n",
    "                        'eMail', 'E-Mail', 'EMAIL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_uniform = email_variations.str.replace(r\"e-?\\s?mail\", \"email\", flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_clean = titles.str.replace(r\"e-?\\s?mail\", \"email\", flags=re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Write a regular expression to extract the domains from `test_urls` and assign the result to `test_urls_clean`. We suggest the following technique:\n",
    "    - Using a series of characters that will match the protocol.\n",
    "    - Inside a capture group, using a set that will match the character classes used in the domain.\n",
    "    - Because all of the URLs either end with the domain, or continue with page path which starts with / (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression.\n",
    "2. Use the same regular expression to extract the domains from the `url` column of the `hn` dataframe. Assign the result to `domains`.\n",
    "3. Use `Series.value_counts()` to build a frequency table of the domains in domains, limiting the frequency table to just to the top 20. Assign the result to `top_domains`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = pd.Series([\n",
    " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
    " 'http://www.interactivedynamicvideo.com/',\n",
    " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
    " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
    " 'HTTPS://github.com/keppel/pinn',\n",
    " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
    " 'https://iot.seeed.cc',\n",
    " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
    " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
    " 'https://www.valid.ly?param'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_d = r\"https?://([\\w.]+)\"\n",
    "test_urls_clean = test_urls.str.extract(pattern_d, flags=re.I)\n",
    "domains = hn['url'].str.extract(pattern_d, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_domains = domains[0].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Write a regular expression that extracts URL components using three capture groups:\n",
    "    - The first capture group should include the protocol text, up to but not including ://.\n",
    "    - The second group should contain the domain, from after :// up to but not including /.\n",
    "    - The third group should contain the page path, from after / to the end of the string.\n",
    "2. Use the regular expression pattern to extract the URL components from the test_urls series. Assign the results to `test_url_parts`.\n",
    "3. Use the regular expression pattern to extract the URL components from the url column of the hn dataframe. Assign the results to `url_parts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = pd.Series([\n",
    " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
    " 'http://www.interactivedynamicvideo.com/',\n",
    " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
    " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
    " 'HTTPS://github.com/keppel/pinn',\n",
    " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
    " 'https://iot.seeed.cc',\n",
    " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
    " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
    " 'https://www.valid.ly?param'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `test_urls` is available from the previous screen\n",
    "pattern = r\"(.+)://([\\w\\.]+)/?(.*)\"\n",
    "\n",
    "test_url_parts = test_urls.str.extract(pattern)\n",
    "url_parts = hn.url.str.extract(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We have provided the regex pattern from the previous screen's solution.\n",
    "\n",
    "1. Uncomment the regular expression pattern. Add names to each capture group:\n",
    "    - The first capture group should be called `protocol`.\n",
    "    - The second capture group should be called `domain`.\n",
    "    - The third capture group should be called `path`.\n",
    "2. Use the regular expression pattern to extract three named columns of url components from the `url` column of the `hn` dataframe. Assign the result to `url_parts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?P<protocol>.+)://(?P<domain>[\\w\\.]+)/?(?P<path>.*)\"\n",
    "\n",
    "test_url_parts = test_urls.str.extract(pattern)\n",
    "url_parts = hn.url.str.extract(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protocol</th>\n",
       "      <th>domain</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http</td>\n",
       "      <td>www.interactivedynamicvideo.com</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http</td>\n",
       "      <td>www.thewire.com</td>\n",
       "      <td>entertainment/2013/04/florida-djs-april-fools-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>www.amazon.com</td>\n",
       "      <td>Technology-Ventures-Enterprise-Thomas-Byers/dp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "      <td>2007/11/07/movies/07stein.html?_r=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http</td>\n",
       "      <td>arstechnica.com</td>\n",
       "      <td>business/2015/10/comcast-and-other-isps-boost-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  protocol                           domain  \\\n",
       "0     http  www.interactivedynamicvideo.com   \n",
       "1     http                  www.thewire.com   \n",
       "2    https                   www.amazon.com   \n",
       "3     http                  www.nytimes.com   \n",
       "4     http                  arstechnica.com   \n",
       "\n",
       "                                                path  \n",
       "0                                                     \n",
       "1  entertainment/2013/04/florida-djs-april-fools-...  \n",
       "2  Technology-Ventures-Enterprise-Thomas-Byers/dp...  \n",
       "3                2007/11/07/movies/07stein.html?_r=0  \n",
       "4  business/2015/10/comcast-and-other-isps-boost-...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_parts.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
